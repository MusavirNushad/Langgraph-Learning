{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229fa157",
   "metadata": {},
   "source": [
    "## X (Twitter) Post Generator Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0bd562c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from typing import TypedDict, Literal, Annotated\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import operator\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8ca6b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3569bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Hugging Face token from the environment\n",
    "hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "# Check if token is loaded properly\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HUGGINGFACE_HUB_TOKEN is not set in the environment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1b6c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm  = HuggingFaceEndpoint(\n",
    "    repo_id=\"moonshotai/Kimi-K2-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=hf_token,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d2a26b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wrap in chat interface\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "345dd062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State definition\n",
    "class TweetState(TypedDict):\n",
    "    topic: str\n",
    "    tweet: str\n",
    "    evaluation: Literal[\"approved\", \"needs_improvement\"]\n",
    "    feedback: str\n",
    "    iteration: int\n",
    "    max_iteration: int\n",
    "    tweet_history: Annotated[list[str], operator.add]\n",
    "    feedback_history: Annotated[list[str], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45be4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_tweet(state: TweetState):\n",
    "    \"\"\"Generate initial tweet\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a funny and clever Twitter/X influencer.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Write a short, original, and hilarious tweet on: \"{state['topic']}\"\n",
    "\n",
    "Rules:\n",
    "- NO question-answer format\n",
    "- Max 800 characters\n",
    "- Use observational humor, irony, sarcasm\n",
    "- Simple, everyday English\n",
    "- Think meme logic and punchlines\n",
    "\n",
    "Just return the tweet text, nothing else.\n",
    "\"\"\")\n",
    "    ]\n",
    "    \n",
    "    response = chat_model.invoke(messages).content.strip()\n",
    "    print(f\"Generated tweet: {response}\")\n",
    "    return {'tweet': response, 'tweet_history': [response]}\n",
    "\n",
    "def evaluate_tweet(state: TweetState):\n",
    "    \"\"\"Evaluate tweet quality\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a Twitter critic evaluating tweets for humor and virality.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Evaluate this tweet: \"{state['tweet']}\"\n",
    "\n",
    "Criteria:\n",
    "1. Originality - Fresh or overused?\n",
    "2. Humor - Actually funny?\n",
    "3. Punchiness - Short and sharp?\n",
    "4. Virality - Shareable?\n",
    "5. Format - Well-formed tweet under 280 chars?\n",
    "\n",
    "Auto-reject if:\n",
    "- Question-answer format\n",
    "- Over 600 characters\n",
    "- Traditional setup-punchline\n",
    "- Weak ending\n",
    "\n",
    "Respond with ONLY:\n",
    "evaluation: approved OR needs_improvement\n",
    "feedback: [one paragraph explanation]\n",
    "\"\"\")\n",
    "    ]\n",
    "    \n",
    "    response = chat_model.invoke(messages).content.strip()\n",
    "    \n",
    "    # Parse response\n",
    "    evaluation = \"needs_improvement\"  # default\n",
    "    feedback = \"Could not parse evaluation\"\n",
    "    \n",
    "    if \"approved\" in response.lower():\n",
    "        evaluation = \"approved\"\n",
    "    \n",
    "    # Extract feedback (everything after \"feedback:\")\n",
    "    if \"feedback:\" in response.lower():\n",
    "        feedback = response.split(\"feedback:\")[-1].strip()\n",
    "    else:\n",
    "        feedback = response\n",
    "    \n",
    "    return {\n",
    "        'evaluation': evaluation, \n",
    "        'feedback': feedback, \n",
    "        'feedback_history': [feedback]\n",
    "    }\n",
    "\n",
    "def optimize_tweet(state: TweetState):\n",
    "    \"\"\"Improve tweet based on feedback\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You improve tweets for virality and humor.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Improve this tweet based on feedback:\n",
    "\n",
    "Topic: \"{state['topic']}\"\n",
    "Current Tweet: \"{state['tweet']}\"\n",
    "Feedback: \"{state['feedback']}\"\n",
    "\n",
    "Write a better version. Avoid Q&A format, stay under 600 characters.\n",
    "Return only the improved tweet.\n",
    "\"\"\")\n",
    "    ]\n",
    "    \n",
    "    response = chat_model.invoke(messages).content.strip()\n",
    "    iteration = state['iteration'] + 1\n",
    "    \n",
    "    return {\n",
    "        'tweet': response, \n",
    "        'iteration': iteration, \n",
    "        'tweet_history': [response]\n",
    "    }\n",
    "\n",
    "def route_evaluation(state: TweetState):\n",
    "    \"\"\"Route based on evaluation result\"\"\"\n",
    "    if state['evaluation'] == 'approved' or state['iteration'] >= state['max_iteration']:\n",
    "        return 'approved'\n",
    "    return 'needs_improvement'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aee0b49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build workflow\n",
    "graph = StateGraph(TweetState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node('generate', generate_tweet)\n",
    "graph.add_node('evaluate', evaluate_tweet)\n",
    "graph.add_node('optimize', optimize_tweet)\n",
    "\n",
    "# Add edges\n",
    "graph.add_edge(START, 'generate')\n",
    "graph.add_edge('generate', 'evaluate')\n",
    "graph.add_conditional_edges(\n",
    "    'evaluate', \n",
    "    route_evaluation, \n",
    "    {'approved': END, 'needs_improvement': 'optimize'}\n",
    ")\n",
    "graph.add_edge('optimize', 'evaluate')\n",
    "\n",
    "# Compile workflow\n",
    "workflow = graph.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57659867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tweet: Jannik Sinner’s forehand is so clean it does its own laundry, folds itself, and still has time to make the rest of the ATP look like they’re swinging a pool noodle.\n",
      "Generated Tweets:\n",
      "1. Jannik Sinner’s forehand is so clean it does its own laundry, folds itself, and still has time to make the rest of the ATP look like they’re swinging a pool noodle.\n",
      "\n",
      "Final Status: approved\n",
      "Iterations: 1\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    initial_state = {\n",
    "        \"topic\": \"Jannik Sinner\",\n",
    "        \"iteration\": 1,\n",
    "        \"max_iteration\": 3\n",
    "    }\n",
    "    \n",
    "    result = workflow.invoke(initial_state)\n",
    "    \n",
    "    print(\"Generated Tweets:\")\n",
    "    for i, tweet in enumerate(result['tweet_history'], 1):\n",
    "        print(f\"{i}. {tweet}\")\n",
    "    \n",
    "    print(f\"\\nFinal Status: {result['evaluation']}\")\n",
    "    print(f\"Iterations: {result['iteration']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
